{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pickle\n",
    "import ftfy\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pandas.read_csv(\"/home/dzhelonkin/bigartm/preprocessed/no_spec_topic_articles.csv\", encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/dzhelonkin/bigartm/preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# process authors\n",
    "authors = data['author']\n",
    "authors = [ftfy.fix_text(i, normalization='NFKC') for i in authors]\n",
    "authors = [re.sub('[^0-9a-zA-Z ;]+', '', str(s)) for s in authors]\n",
    "authors = [i.lower().split('; ') for i in authors]\n",
    "authors = [' '.join([au.replace(' ', '_') for au in au_text]) for au_text in authors]\n",
    "with open(path+\"authors.txt\", 'w') as f:\n",
    "   for a in authors:\n",
    "       f.write(a+'\\n')\n",
    "with open(path+\"authors.pickle2\", 'wb') as f:\n",
    "   pickle.dump(authors, f, protocol=2)\n",
    "with open(path+\"authors.pickle3\", 'wb') as f:\n",
    "   pickle.dump(authors, f, protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(''' -[0-9a-zA-Z]+''', '', ' -xray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(data, name, save=True):\n",
    "    print('work with', name)\n",
    "    print('ftfy stage')\n",
    "    tmp = [ftfy.fix_text(i, normalization='NFKC') for i in data]\n",
    "    print('regexp stage')\n",
    "    tmp = [re.sub('-', '', s) for s in tmp]\n",
    "    tmp = [re.sub(r'''[^0-9a-zA-Z]+''', ' ', s) for s in tmp]\n",
    "    tmp = [re.sub(' [0-9]+', ' ', s) for s in tmp]\n",
    "    tmp = [re.sub(r' [a-zA-Z] ', ' ', s) for s in tmp]\n",
    "    tmp = [re.sub(r' [ ]+', ' ', s) for s in tmp]\n",
    "    stop = set(stopwords.words('english'))\n",
    "    ans = ['' for i in tmp]\n",
    "    c = -1\n",
    "    wnl = WordNetLemmatizer()\n",
    "    print('stopwords and WordNetLemmatizer stage')\n",
    "    for t in tmp:\n",
    "        c +=1\n",
    "        if c%3000 == 0:\n",
    "            print(c)\n",
    "        for w in t.split(' '):\n",
    "            if w != '':\n",
    "                w = w.lower()\n",
    "                if w not in stop:\n",
    "                    ans[c] += wnl.lemmatize(w)+' '\n",
    "\n",
    "    if save:\n",
    "        print('save stage')\n",
    "        with open(path+name+\".pickle2\", 'wb') as f:\n",
    "            pickle.dump(ans, f, protocol=2)\n",
    "        with open(path+name+\".pickle3\", 'wb') as f:\n",
    "            pickle.dump(ans, f, protocol=3)\n",
    "        with open(path+name+\".txt\", 'w') as f:\n",
    "            for text in ans:\n",
    "                f.write(text+'\\n')\n",
    "            \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work with bodies\n",
      "ftfy stage\n",
      "regexp stage\n",
      "stopwords and WordNetLemmatizer stage\n",
      "0\n",
      "3000\n",
      "6000\n",
      "9000\n",
      "12000\n",
      "15000\n",
      "18000\n",
      "save stage\n"
     ]
    }
   ],
   "source": [
    "# process bodies\n",
    "bodies = data['body']\n",
    "pattern_delim = re.compile(r'\\[.+?\\]')\n",
    "texts = [pattern_delim.sub('', i) for i in bodies]\n",
    "pattern_delim = re.compile(r'\\(.+?\\)')\n",
    "texts = [pattern_delim.sub('', i) for i in texts]\n",
    "ans = process(data=texts, name='bodies', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work with abstracts\n",
      "ftfy stage\n",
      "regexp stage\n",
      "stopwords and WordNetLemmatizer stage\n",
      "0\n",
      "3000\n",
      "6000\n",
      "9000\n",
      "12000\n",
      "15000\n",
      "18000\n",
      "save stage\n"
     ]
    }
   ],
   "source": [
    "# process abstracts\n",
    "ans = process(data=data['abstract'], name='abstracts', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work with titles\n",
      "ftfy stage\n",
      "regexp stage\n",
      "stopwords and WordNetLemmatizer stage\n",
      "0\n",
      "3000\n",
      "6000\n",
      "9000\n",
      "12000\n",
      "15000\n",
      "18000\n",
      "save stage\n"
     ]
    }
   ],
   "source": [
    "# process titles\n",
    "ans = process(data=data['title'], name='titles', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
